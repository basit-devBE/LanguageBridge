# My Journey Building an AI Translation System: From Azubi Cloud Engineering Student to Production

*How I tackled a real-world cloud project as part of my journey into cloud engineering*

## Introduction

Over the past few weeks, I got accepted into the **Azubi Talent Mobility Program (Cloud Engineering Track)**. This was a huge milestone for me as a junior Cloud and Software Engineer trying to gain real-world experience before graduating from school.

As part of our core projects, I was tasked with building an **AI automation system** that:
- Takes files (JSON files) or text inputs
- Uploads them to Amazon S3
- Triggers a Lambda function 
- Uses Amazon Translate to translate the content
- Returns the translated results

This challenge excited me because it combined multiple AWS services and gave me hands-on experience with serverless architecture. Let me walk you through my journey and the solution I built.

## ğŸ—ï¸ My Architecture Solution

After analyzing the requirements, I designed this serverless architecture:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚ -> â”‚   API Gateway   â”‚ -> â”‚   Lambda        â”‚ -> â”‚      S3         â”‚
â”‚ (Frontend/API)  â”‚    â”‚   (REST API)    â”‚    â”‚  (Node.js API)  â”‚    â”‚   (Storage)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚                       â”‚
                                                         v                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚                       â”‚
â”‚    Response     â”‚ <- â”‚   Lambda        â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚   (Translated)  â”‚    â”‚ (Python Translate) â”‚ <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              S3 Event Trigger
```

### How It Works:
1. **ğŸŒ API Gateway**: Receives translation requests from clients
2. **âš¡ Lambda (Node.js)**: Processes requests, validates input, handles file uploads
3. **ğŸ“¦ S3 Storage**: Stores translation requests and triggers the next step
4. **ğŸ¤– Lambda (Python)**: Gets triggered by S3 events, performs actual translation using AWS Translate
5. **ï¿½ Response**: Results are stored back in S3 and can be retrieved via API

This event-driven approach was perfect for the requirements - it's scalable, cost-effective, and handles the async nature of translation processing beautifully.

## ğŸš€ Setting Up My Development Environment

As a student in the Azubi Cloud Engineering program, I had to make sure my setup was solid before diving into the code:

### What I Needed:
- AWS Account (provided through the program)
- Node.js and npm for the API development
- AWS CLI configured with my credentials
- Terraform for infrastructure management (this was new to me!)

### My Project Structure:
I organized my code following best practices I learned in the program:

```
AI_Automation/
â”œâ”€â”€ api/                     # My Node.js API code
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.ts          # Main translation handlers
â”‚   â”‚   â”œâ”€â”€ aws.ts           # AWS service integrations  
â”‚   â”‚   â””â”€â”€ routes.ts        # API routing logic
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”œâ”€â”€ infrastructure/          # Terraform and deployment
â”‚   â”œâ”€â”€ main.tf              # Infrastructure as code
â”‚   â””â”€â”€ index.js             # Lambda handler adapter
â”œâ”€â”€ lambda/                  # Python translation service
â”‚   â””â”€â”€ translation_handler.py
â””â”€â”€ tests/                   # My testing scripts
    â””â”€â”€ test_serverless_api.sh
```

Setting up this structure helped me stay organized as the project grew more complex.

## ğŸ’» Building the Solution

### 1. Creating My Node.js API Handler

The first challenge was building an API that could handle both text and file translations. Coming from a traditional web development background, I had to learn how to adapt Express-style code for AWS Lambda:

```typescript
// api/src/main.ts - My main translation handler
export const translateTextHandler = async (req: Request, res: Response) => {
    try {
        const { text, sourceLanguage = 'auto', targetLanguage = 'en', waitForResult = true } = req.body;
        
        // Input validation - learned this the hard way during testing!
        if (!text) {
            return res.status(400).json({ error: 'Text is required' });
        }

        const requestId = uuidv4();
        const translationRequest = {
            request_id: requestId,
            text: text,
            source_language: sourceLanguage,
            target_language: targetLanguage,
            timestamp: new Date().toISOString(),
            type: 'text_translation'
        };

        // Upload request to S3 - this triggers our Python Lambda
        await uploadTranslationRequest(translationRequest, requestId);
        
        if (!waitForResult) {
            // Return immediately for async processing
            return res.status(202).json({
                message: 'Translation request submitted successfully',
                requestId: requestId,
                status: 'processing'
            });
        }

        // Wait for translation to complete (for sync requests)
        const result = await waitForTranslationResult(requestId, 30);
        
        if (result) {
            return res.status(200).json({
                message: 'Translation completed successfully',
                requestId: requestId,
                status: 'completed',
                result: result
            });
        }
    } catch (error) {
        console.error('Translation error:', error);
        return res.status(500).json({ error: 'Failed to process translation' });
    }
};
```

**Key Learning**: I initially struggled with async processing. My mentor at Azubi helped me understand that not every API call needs to wait for completion - sometimes it's better to return immediately and let users check status later.

### 2. The Lambda Adapter Challenge

One of the biggest hurdles I faced was making my Express-style API work with AWS Lambda. Lambda doesn't understand Express requests and responses directly, so I had to create an adapter:

```javascript
// infrastructure/index.js - My Lambda adapter
class MockRequest {
    constructor(event) {
        this.body = event.body ? JSON.parse(event.body) : {};
        this.params = event.pathParameters || {};
        this.query = event.queryStringParameters || {};
        this.headers = event.headers || {};
        this.method = event.httpMethod;
        this.path = event.path;
        this.file = null; // For file uploads
    }
}

class MockResponse {
    constructor() {
        this.statusCode = 200;
        this.responseBody = {};
        this.headers = {
            'Content-Type': 'application/json',
            'Access-Control-Allow-Origin': '*',  // Learned about CORS the hard way!
            'Access-Control-Allow-Headers': 'Content-Type',
            'Access-Control-Allow-Methods': 'GET, POST, OPTIONS'
        };
    }

    status(code) {
        this.statusCode = code;
        return this;
    }

    json(body) {
        this.responseBody = body;
        return this;
    }

    toAPIGatewayResponse() {
        return {
            statusCode: this.statusCode,
            headers: this.headers,
            body: JSON.stringify(this.responseBody)
        };
    }
}

exports.handler = async (event, context) => {
    try {
        const req = new MockRequest(event);
        const res = new MockResponse();
        
        const requestPath = event.path;
        const method = event.httpMethod;
        
        // Route to the right handler based on path and method
        if (requestPath === '/translate/text' && method === 'POST') {
            await translateTextHandler(req, res);
        } else if (requestPath === '/translate/file' && method === 'POST') {
            await handleFileUpload(req);  // Handle multipart uploads
            await translateFileHandler(req, res);
        } else if (requestPath.startsWith('/translate/status/') && method === 'GET') {
            await getTranslationStatusHandler(req, res);
        } else {
            res.status(404).json({ error: 'Endpoint not found' });
        }
        
        return res.toAPIGatewayResponse();
    } catch (error) {
        console.error('Lambda handler error:', error);
        return {
            statusCode: 500,
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ error: 'Internal Server Error' })
        };
    }
};
```

**The CORS Struggle**: I spent hours debugging why my frontend couldn't call the API, only to discover I needed proper CORS headers. This taught me the importance of understanding browser security!

### 3. Learning Infrastructure as Code with Terraform

Before this project, I had never used Terraform. As part of the Azubi program, we were encouraged to use Infrastructure as Code practices. Here's what I learned:

```hcl
# infrastructure/main.tf - My first Terraform configuration!
resource "aws_lambda_function" "api_handler" {
  filename         = "nodejs-api-lambda.zip"
  function_name    = "ai-automation-nodejs-api-handler"
  role            = aws_iam_role.api_lambda_role.arn
  handler         = "index.handler"
  runtime         = "nodejs18.x"
  timeout         = 30
  memory_size     = 512

  environment {
    variables = {
      AWS_S3_BUCKET_NAME = data.aws_s3_bucket.existing_bucket.id
      NODE_ENV          = "production"
    }
  }
}

resource "aws_api_gateway_rest_api" "translation_api" {
  name        = "ai-automation-api"
  description = "My AI Translation System - Azubi Project"
  
  endpoint_configuration {
    types = ["REGIONAL"]
  }
}

resource "aws_api_gateway_resource" "translate_text_resource" {
  rest_api_id = aws_api_gateway_rest_api.translation_api.id
  parent_id   = aws_api_gateway_resource.translate_resource.id
  path_part   = "text"
}

resource "aws_api_gateway_method" "translate_text_post" {
  rest_api_id   = aws_api_gateway_rest_api.translation_api.id
  resource_id   = aws_api_gateway_resource.translate_text_resource.id
  http_method   = "POST"
  authorization = "NONE"
}
```

**Terraform Lessons**: Initially, I tried to create everything manually in the AWS console. My instructor showed me how Terraform makes infrastructure reproducible and version-controlled. Game changer!

## ğŸ”„ The Event-Driven Magic

The most elegant part of my solution was the event-driven architecture. Here's how the translation pipeline works:

1. **ğŸ“¤ Request Submission**: My API receives a translation request
2. **ğŸ“ S3 Upload**: The request gets stored in S3 under the `requests/` prefix
3. **âš¡ Automatic Trigger**: S3 automatically triggers my Python Lambda function
4. **ğŸ¤– Translation**: Python Lambda uses AWS Translate to do the actual work
5. **ğŸ’¾ Result Storage**: The translated result gets stored back in S3 under `responses/`
6. **ğŸ“Š Status Check**: Users can check results via my status endpoint

### The Python Translation Worker

This was my first time working with AWS Translate. The Python Lambda was surprisingly straightforward:

```python
# lambda/translation_handler.py - The worker that does the actual translation
import json
import boto3
from datetime import datetime

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    translate = boto3.client('translate')
    
    # Process each file uploaded to S3
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        
        # Get the translation request from S3
        response = s3.get_object(Bucket=bucket, Key=key)
        request_data = json.loads(response['Body'].read())
        
        # This is where the magic happens!
        result = translate.translate_text(
            Text=request_data['text'],
            SourceLanguageCode=request_data['source_language'],
            TargetLanguageCode=request_data['target_language']
        )
        
        # Package the response
        response_data = {
            'requestId': request_data['request_id'],
            'originalText': request_data['text'],
            'translatedText': result['TranslatedText'],
            'sourceLanguage': result['SourceLanguageCode'],
            'targetLanguage': result['TargetLanguageCode'],
            'timestamp': datetime.utcnow().isoformat(),
            'status': 'completed'
        }
        
        # Save the result back to S3
        s3.put_object(
            Bucket=bucket,
            Key=f"responses/{request_data['request_id']}.json",
            Body=json.dumps(response_data),
            ContentType='application/json'
        )
    
    return {'statusCode': 200}
```

**What I Learned**: Event-driven architecture is powerful! Instead of having my API wait for translation to complete, I let S3 events trigger the process automatically. This makes the system much more scalable.

## ğŸ§ª Testing My Creation

Once everything was deployed, I was excited to test my system! Here's how I validated that everything worked:

### Testing Text Translation

```bash
curl -X POST 'https://rqs7gp9xyf.execute-api.eu-north-1.amazonaws.com/prod/translate/text' \
  -H 'Content-Type: application/json' \
  -d '{
    "text": "Hello, how are you today?",
    "sourceLanguage": "en",
    "targetLanguage": "es",
    "waitForResult": false
  }'
```

Response:
```json
{
  "message": "Translation request submitted successfully",
  "requestId": "9c45f245-39f5-4835-a2f6-9526ad268f65",
  "status": "processing"
}
```

### Testing JSON File Translation

This was the cool part - translating entire JSON configuration files:

```bash
curl -X POST 'https://rqs7gp9xyf.execute-api.eu-north-1.amazonaws.com/prod/translate/file' \
  -H 'Content-Type: application/json' \
  -d '{
    "fileContent": "{\"welcome\": \"Welcome to our app!\", \"goodbye\": \"See you later!\"}",
    "fileName": "messages.json",
    "sourceLanguage": "en",
    "targetLanguage": "es",
    "waitForResult": false
  }'
```

### Checking Translation Status

```bash
curl -s 'https://rqs7gp9xyf.execute-api.eu-north-1.amazonaws.com/prod/translate/status/9c45f245-39f5-4835-a2f6-9526ad268f65'
```

Response:
```json
{
  "message": "Translation completed",
  "requestId": "9c45f245-39f5-4835-a2f6-9526ad268f65",
  "status": "completed",
  "result": {
    "originalText": "Hello, how are you today?",
    "translatedText": "Hola, Â¿cÃ³mo estÃ¡s hoy?",
    "sourceLanguage": "en",
    "targetLanguage": "es"
  }
}
```

**The Moment of Truth**: When I saw that first successful translation come back, I felt like I had really accomplished something! All those hours of debugging AWS permissions and Lambda timeouts were worth it.

## ğŸ“Š My System in Action - Real Demo Results

I created a demo script to showcase my system to my Azubi instructors. Here's what a live run looked like:

```bash
ğŸš€ AI Translation System - Student Project Demo
=============================================

ğŸ“¡ Step 1: Submit Translation Request
--------------------------------------
ğŸ’¬ Text: 'Hello world, how are you today?'
ğŸŒ Direction: English â†’ Spanish

ğŸ“¤ API Response:
{
  "message": "Translation request submitted successfully",
  "requestId": "5c88f110-8fc2-4954-9573-a59b0e43aecd",
  "status": "processing"
}

ğŸ“Š Step 2: My System Working Behind the Scenes
----------------------------------------------
ğŸ“¥ Recent translation requests in S3:
2025-08-06 19:56:45  502 requests/e5dde0ab-0016-4ad4-8927-b6cffd5684ca_1754510204853.json
2025-08-06 19:47:59  231 requests/f60054de-5b30-44a2-901e-7268f7f40b47_1754509678133.json

âœ… Recent completed translations in S3:
2025-08-06 19:48:00  322 responses/fa67befc-a847-4cfd-9543-403ff0f345b0.json
2025-08-06 19:48:01  309 responses/fc05f304-c937-45e9-a290-9ef6c19deb6a.json

ğŸ¯ Step 3: The Final Translation Result
---------------------------------------
ğŸ“‹ Translation Result:
{
  "requestId": "fc05f304-c937-45e9-a290-9ef6c19deb6a",
  "type": "text",
  "originalText": "Hola, Â¿cÃ³mo estÃ¡s?",
  "translatedText": "Hello, how are you?",
  "sourceLanguage": "es",
  "targetLanguage": "en",
  "timestamp": "2025-08-06T19:48:00.338584",
  "status": "completed"
}
```

**Presentation Success**: When I showed this to my instructors and fellow students, they were impressed by how quickly the system processed requests and how clean the S3-based workflow was!

## ğŸ—ï¸ Deployment Journey

### 1. Building My First Lambda Package

As a student, packaging code for Lambda was completely new to me. Here's the build script I created:

```bash
#!/bin/bash
# My first Lambda deployment script!

echo "ğŸ”¨ Building Node.js API for Lambda deployment..."

# Create temporary build directory
BUILD_DIR="/tmp/nodejs-lambda-build-$(date +%s)"
mkdir -p "$BUILD_DIR"

# Copy my source files
cp -r api/src "$BUILD_DIR/"
cp api/package.json "$BUILD_DIR/"
cp api/tsconfig.json "$BUILD_DIR/"
cp index.js "$BUILD_DIR/"

cd "$BUILD_DIR"

# Install production dependencies only
npm install --production

# Compile my TypeScript code
npm install -g typescript
tsc

# Create the deployment package
zip -r nodejs-api-lambda.zip . -x "*.ts" "tsconfig.json"

# Move to infrastructure directory
mv nodejs-api-lambda.zip /home/basit/Desktop/github_projects/AI_Automation/infrastructure/

echo "âœ… Build complete! Ready for deployment."
```

### 2. My First Terraform Deployment

```bash
cd infrastructure
export AWS_DEFAULT_REGION=eu-north-1
terraform init
terraform plan    # I always check what will be created first!
terraform apply -auto-approve
```

### 3. Updating My Lambda Function

```bash
aws lambda update-function-code \
  --function-name ai-automation-nodejs-api-handler \
  --zip-file fileb://nodejs-api-lambda.zip
```

**Deployment Challenges**: I initially forgot to set the correct AWS region and spent an hour wondering why my resources weren't appearing in the console! Learning experience!

## ğŸ¯ What I Accomplished

### âœ… **Features I Built:**
- **Async Processing**: Users don't have to wait for translations to complete
- **JSON File Translation**: Perfect for internationalizing app configurations
- **Status Tracking**: Real-time status checking via API
- **Error Handling**: Proper error responses and validation
- **CORS Support**: Frontend-friendly API
- **Multi-language Support**: 75+ language pairs through AWS Translate
- **Serverless Scaling**: Handles multiple requests automatically

### ğŸ”§ **Technical Skills I Developed:**
- **TypeScript**: First time using it for backend development
- **AWS Lambda**: Event-driven serverless computing
- **Infrastructure as Code**: Terraform for managing AWS resources
- **Event-Driven Architecture**: S3 triggers and async processing
- **API Design**: RESTful endpoints with proper HTTP status codes
- **Cloud Security**: IAM roles and least privilege access

### ğŸ“ˆ **System Performance:**
- **Response Time**: ~200ms for request submission
- **Processing Time**: 2-5 seconds for translation completion
- **Cost**: Under $1/month for my development testing
- **Scalability**: Can handle hundreds of concurrent requests
- **Supported Languages**: 75+ language pairs

**My Biggest Win**: Building a production-ready API that my classmates are actually using for their own projects!

## ğŸ”® What's Next for My Project

Now that I have a working system, I'm planning these enhancements:

1. **Real-time WebSocket Updates**: Push notifications when translations complete
2. **Batch File Processing**: Upload multiple files at once
3. **Custom Translation Models**: Train domain-specific models for better accuracy
4. **User Authentication**: Add API keys for usage tracking
5. **Dashboard**: Web interface to monitor translations and usage
6. **Rate Limiting**: Prevent abuse and manage costs

## ğŸ’¡ What I Learned as a Student

### What Worked Really Well:
- **Starting Simple**: I began with just text translation, then added files
- **Event-Driven Design**: S3 triggers made the architecture elegant
- **TypeScript**: Caught so many errors before deployment!
- **Infrastructure as Code**: Terraform made deployments repeatable

### Challenges I Overcame:
- **Lambda Cold Starts**: Learned to optimize package size and memory
- **Async Processing**: Understanding when to wait vs. when to return immediately
- **AWS Permissions**: IAM roles were confusing at first, but crucial for security
- **CORS Issues**: Browser security was a mystery until I learned about it

### Advice for Other Students:
- **Start with the AWS Free Tier**: Perfect for learning and experimenting
- **Use CloudWatch Logs**: Essential for debugging Lambda functions
- **Test Early, Test Often**: Deploy small changes frequently
- **Ask for Help**: The Azubi community and instructors are incredibly supportive

## ğŸ‰ Reflecting on My Journey

Building this AI translation system has been one of the most rewarding experiences of my time in the Azubi Talent Mobility Program. When I started, I barely knew what Lambda was, and now I've built a complete serverless application that my classmates are using!

The project taught me:
- **Cloud-Native Thinking**: How to design for scalability from day one
- **Modern Development Practices**: TypeScript, Infrastructure as Code, async patterns
- **Production Readiness**: Error handling, monitoring, and proper testing

Most importantly, it gave me confidence that I can tackle complex cloud engineering challenges. The transition from traditional web development to serverless architecture was challenging, but the Azubi program provided the perfect environment to learn and grow.

## ğŸ”— My Project Links

- **GitHub Repository**: [LanguageBridge](https://github.com/basit-devBE/LanguageBridge)
- **Live API Endpoint**: `https://rqs7gp9xyf.execute-api.eu-north-1.amazonaws.com/prod`
- **Azubi Africa**: [Talent Mobility Program](https://azubiafrica.org/)

---

*Ready to start your own cloud journey? The Azubi Talent Mobility Program is accepting applications! ğŸš€*

## ğŸ“ Quick API Reference for Fellow Students

### Available Endpoints

| Method | Endpoint | What It Does |
|--------|----------|-------------|
| POST | `/translate/text` | Translate text input |
| POST | `/translate/file` | Translate JSON file content |
| GET | `/translate/status/{id}` | Check translation status |

### Example Usage

#### Translate Text
```json
POST /translate/text
{
  "text": "Hello world",
  "sourceLanguage": "en",
  "targetLanguage": "es",
  "waitForResult": false
}
```

#### Response
```json
{
  "message": "Translation request submitted successfully",
  "requestId": "uuid-here",
  "status": "processing"
}
```

#### Check Status
```json
GET /translate/status/uuid-here
{
  "message": "Translation completed",
  "requestId": "uuid-here",
  "status": "completed",
  "result": {
    "originalText": "Hello world",
    "translatedText": "Hola mundo",
    "sourceLanguage": "en",
    "targetLanguage": "es"
  }
}
```

Feel free to try it out and let me know what you think! ğŸ“§
